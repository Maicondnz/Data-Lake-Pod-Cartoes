# **PoD CartÃµes - Projeto Data Lake**

## ğŸ“Œ**IntroduÃ§Ã£o**
A PoD CartÃµes Ã© uma empresa de cartÃµes de crÃ©dito que busca otimizar o uso de seus dados, porÃ©m enfrenta desafios devido Ã  fragmentaÃ§Ã£o das informaÃ§Ãµes em mÃºltiplos sistemas lentos e a uma infraestrutura que nÃ£o suporta Big Data. Essa limitaÃ§Ã£o dificulta o consumo de dados organizados e de alta qualidade, prejudicando o desenvolvimento de modelos preditivos. Para solucionar esse problema, a empresa pretende implementar um Data Lake escalÃ¡vel e acessÃ­vel, garantindo a unificaÃ§Ã£o, governanÃ§a e seguranÃ§a dos dados. AlÃ©m disso, serÃ¡ desenvolvido um Book de VariÃ¡veis para apoiar a criaÃ§Ã£o de modelos analÃ­ticos mais eficazes.


A soluÃ§Ã£o utiliza serviÃ§os da AWS para ingestÃ£o, processamento e organizaÃ§Ã£o de dados em zonas especÃ­ficas (Raw, Trusted e Curated), alÃ©m de orquestraÃ§Ã£o de pipelines com o **Apache Airflow**.

## ğŸ“Œ**Dados**
![dados relacionamento](imgs/dados.jpg)


## ğŸ“Œ**Arquitetura**
A arquitetura do projeto estÃ¡ ilustrada abaixo:

![arquitetura](imgs/Arquitetura.png)

## ğŸ“Œ**Data Lake Zonas**

| **Zona**      | **DescriÃ§Ã£o**                                                                                                                                                                                                                                                                          |
|---------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Ingestion** | Dados brutos coletados diretamente do sistema transacional.                                                                                                                                                          |
| **Raw**       | Dados prÃ©-processados que passaram por uma limpeza e validaÃ§Ã£o iniciais, particionados por data e armazenados em formato Parquet. Uma tabela de controle Ã© criada para registrar a data de processamento..                           |
| **Curated**   | Dados refinados, deduplicados e integrados, com enriquecimento por meio da criaÃ§Ã£o de variÃ¡veis customizadas alinhadas aos requisitos do negÃ³cio. Essa camada inclui uma etapa prÃ©-book (stage) para desenvolvimento de variÃ¡veis, seguida da agregaÃ§Ã£o final conforme os perÃ­odos de anÃ¡lise e armazenamento na camada de Book para anÃ¡lises avanÃ§adas e modelagem preditiva. |
                                                                                                                
                                                                                                                
                                                                                                                
## ğŸ“Œ**ServiÃ§os Utilizados**

| **ServiÃ§o**         | **DescriÃ§Ã£o**                                                                                                                                                      |
|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **EC2**  |  InstÃ¢ncia utilizada para executar o Apache Airflow em um ambiente Docker e a ingestÃ£o dos dados.                        |
| **Apache Airflow**  | OrquestraÃ§Ã£o de pipelines de ingestÃ£o e transformaÃ§Ã£o de dados, permitindo o agendamento, monitoramento e automaÃ§Ã£o dos fluxos de trabalho.                        |
| **EMR**             | Plataforma gerenciada de Big Data que faz a execuÃ§Ã£o dos processos ETL e anÃ¡lises distribuÃ­das, utilizando Spark                   |
| **Glue**            | ServiÃ§o para catalogaÃ§Ã£o que automatiza a descoberta dos dados armazenados no S3 para consulta e anÃ¡lise.                             |
| **Athena**          | ServiÃ§o de consulta interativa que permite executar queries SQL diretamente sobre os dados armazenados no S3, facilitando anÃ¡lises ad hoc e geraÃ§Ã£o de relatÃ³rios. |
| **S3**              | Armazenamento escalÃ¡vel e durÃ¡vel que hospeda as zonas Raw, Trusted e Curated do Data Lake.                                                                         |
| **IAM**             | Gerenciamento de identidades e acessos que assegura controles de seguranÃ§a e permissÃµes adequadas aos recursos AWS.                                                   |
| **CloudWatch**      | ServiÃ§o de monitoramento que coleta mÃ©tricas e logs da infraestrutura AWS, contribuindo para o controle de custos e desempenho.                                      |
| **Docker**          | Plataforma de contÃªinerizaÃ§Ã£o que facilita a criaÃ§Ã£o, implantaÃ§Ã£o e execuÃ§Ã£o de aplicaÃ§Ãµes em ambientes isolados e consistentes, otimizando o desenvolvimento e a integraÃ§Ã£o. |

## ğŸ“Œ**Bibliotecas**

- pandas
- pyspark
- datetime
- time
- boto3
- configparser
- os

## ğŸ“Œ**DAGS**
ğŸ”¹ **Todas as Dags sÃ£o programadas para enviar um e-mail em caso de falha nos processos.**

### **IngestÃ£o**
- ResponsÃ¡vel pela extraÃ§Ã£o de dados do SGBD e armazenamento na primeira camada do Data Lake (Ingestion).
- A DAG Ã© programada para executar diariamente Ã s 05:00h, garantindo que os dados mais recentes sejam transferidos para o Data Lake.
  
![dag ingestÃ£o](imgs/dag_ingestion.jpg)
  

### **Processamento de dados por tabela**
- Esta DAG inicia um cluster EMR que executa um step job para realizar o processamento dos dados por assunto (tabelas individuais).
- ApÃ³s o processamento, o cluster Ã© finalizado automaticamente para evitar custos adicionais.
- Programada para executar diariamente Ã s 08:00h, mantendo os dados processados atualizados.
  
![dag processamento](imgs/dag_processamento.jpg)

### **Processamento de Book**
- Focada na criaÃ§Ã£o e atualizaÃ§Ã£o das tabelas stage e book de variÃ¡veis, essenciais para anÃ¡lises e modelos preditivos.
- A DAG sobe um cluster EMR, executa o step job e encerra o cluster ao final do processamento.
- Programada para rodar mensalmente, todo dia 01, Ã s 22:00h, garantindo que os dados do book estejam preparados para anÃ¡lises estratÃ©gicas.
  
![dag book](imgs/dag_book.jpg)

## ğŸ“Œ**BOOK DE VARIÃVEIS**

###  **STAGE**
Na etapa **Stage**, foram criadas variÃ¡veis para analisar o comportamento de uso do cartÃ£o de crÃ©dito pelos clientes. As principais mÃ©tricas incluem:  

- **ClassificaÃ§Ã£o de Dias de Atraso**  
- **NÃºmero de Dias em Atraso**  
- **ClassificaÃ§Ã£o do Valor Pago em RelaÃ§Ã£o Ã  Fatura**  
- **Porcentagem da Fatura Paga**  
- **Quantidade de TransaÃ§Ãµes**  

---

### **BOOK**  
Na etapa **Book**, os valores numÃ©ricos foram agregados por classificaÃ§Ã£o e janelas de tempo, utilizando **01/02/2024** como data de referÃªncia.  
A anÃ¡lise segue uma visÃ£o mensal para os perÃ­odos:  
**U1M, U3M, U6M, U9M e U12M** (Ãºltimos 1, 3, 6, 9 e 12 meses).  

Ao todo, foram geradas **665 variÃ¡veis agregadas**, organizadas conforme as seguintes categorias:  

---

## ğŸ”¹ 1. ClassificaÃ§Ã£o por Prazo de Pagamento (`fbc_classificacao_dias_pagamento`)  
Define o status do pagamento com base na data de vencimento:  

- `SEM_PAGAMENTO` â†’ Nenhum pagamento registrado  
- `PAGAMENTO_ATRASADO` â†’ Pago apÃ³s o vencimento  
- `PAGAMENTO_NO_PRAZO` â†’ Pago exatamente no vencimento  
- `PAGAMENTO_ANTECIPADO` â†’ Pago antes do vencimento  

## ğŸ”¹ 2. ClassificaÃ§Ã£o por Valor Pago (`fbc_classificacao_vlr_pagamento`)  
Agrupa os pagamentos conforme a proporÃ§Ã£o do valor pago em relaÃ§Ã£o ao total da fatura:  

- `PAGAMENTO_INSUFICIENTE` â†’ Pagamento menor que o mÃ­nimo exigido  
- `PAGAMENTO_MINIMO` â†’ Pagamento exatamente no valor mÃ­nimo  
- `PAGAMENTO_TOTAL` â†’ Pagamento integral da fatura  
- `PAGAMENTO_PARCIAL` â†’ Pagamento maior que o mÃ­nimo, mas menor que o total  

## ğŸ”¹ 3. Indicadores Financeiros Calculados (`fvls`)  
Cada mÃ©trica de pagamento Ã© analisada a partir das seguintes variÃ¡veis:  

- ğŸ“Œ **`fvl_valor_fatura`** â†’ Valor total da fatura  
- ğŸ“Œ **`fvl_valor_pagamento_minimo`** â†’ Valor mÃ­nimo exigido  
- ğŸ“Œ **`fvl_valor_pagamento`** â†’ Valor efetivamente pago  
- ğŸ“Œ **`fvl_numero_dias_atraso`** â†’ NÃºmero de dias em atraso  
- ğŸ“Œ **`fvl_qtd_transacao`** â†’ Quantidade de transaÃ§Ãµes realizadas  
- ğŸ“Œ **`fvl_pct_fatura_pgto`** â†’ Percentual da fatura que foi paga  

## ğŸ”¹ 4. Janelas Temporais (`janelas`)  
As mÃ©tricas sÃ£o analisadas dentro das seguintes janelas de tempo:  

- ğŸ•’ **Ãšltimo mÃªs (`flg_u1m`)**  
- ğŸ•’ **Ãšltimos 3 meses (`flg_u3m`)**  
- ğŸ•’ **Ãšltimos 6 meses (`flg_u6m`)**  
- ğŸ•’ **Ãšltimos 9 meses (`flg_u9m`)**  
- ğŸ•’ **Ãšltimos 12 meses (`flg_u12m`)**  

## ğŸ”¹ 5. MÃ©tricas Agregadas (`aggs`)  
Para cada variÃ¡vel financeira e janela temporal, sÃ£o aplicadas as seguintes funÃ§Ãµes estatÃ­sticas:  

- **`SUM`** â†’ Soma dos valores no perÃ­odo  
- **`AVG`** â†’ MÃ©dia dos valores no perÃ­odo  
- **`MAX`** â†’ Valor mÃ¡ximo no perÃ­odo  
- **`MIN`** â†’ Valor mÃ­nimo no perÃ­odo  

---

## ğŸ”¹ 6. Regras de ExclusÃ£o de MÃ©tricas  
Para garantir a coerÃªncia dos cÃ¡lculos, algumas combinaÃ§Ãµes de mÃ©tricas foram desconsideradas:  

âŒ **`fvl_numero_dias_atraso`** â†’ NÃ£o faz sentido somar dias de atraso ao longo dos meses.  

âŒ **`fvl_qtd_transacao`** â†’ SÃ³ pode ser somado, pois cada transaÃ§Ã£o Ã© contabilizada individualmente por mÃªs.  

âŒ **`fvl_pct_fatura_pgto`** â†’ NÃ£o faz sentido somar percentuais de fatura paga ao longo dos meses.  

âŒ **`flg_u1m`** â†’ Permite apenas soma (`SUM`), pois em um Ãºnico mÃªs, as funÃ§Ãµes `SUM`, `AVG`, `MAX` e `MIN` retornariam o mesmo valor.  

âŒ **`SEM_PAGAMENTO` e `PAGAMENTO_TOTAL`** â†’ NÃ£o possuem mÃ©tricas de percentual pago, pois resultariam em colunas constantes.  

---
